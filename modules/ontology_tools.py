#!/usr/bin/env python
# coding: utf8
#from gluon import *  #temporary, until we're ready to integrate
from urllib2 import urlopen
from lxml import etree

## This module is used to load owl format ontologies.  There should probably be a class for returned ontology information, but a dict might suffice


RDF_PREFIX = "{http://www.w3.org/1999/02/22-rdf-syntax-ns#}"
RDFS_PREFIX = "{http://www.w3.org/2000/01/rdf-schema#}"
OWL_PREFIX = "{http://www.w3.org/2002/07/owl#}"
OBO_PURL_PREFIX = "{http://purl.obolibrary.org/obo/}"

OWL_SUFFIX = ".owl"

RDF_RESOURCE = RDF_PREFIX + 'resource'
RDF_ABOUT = RDF_PREFIX + 'about'

RDFS_LABEL = RDFS_PREFIX + 'label'

OWL_CLASS = OWL_PREFIX + 'Class'
OWL_EQUIVALENTCLASS = OWL_PREFIX + 'equivalentClass'
OWL_RESTRICTION = OWL_PREFIX + 'Restriction'
OWL_SUBCLASS_OF = RDFS_PREFIX + 'subClassOf'
IAO_ANNOTATION = OBO_PURL_PREFIX + 'IAO_0000115'

_CONF_OBJ_DICT = {}

def get_conf(request,domain):
    """ 
    Lifted from phylografter's externalproc.py (so presumibly originally
    from Mark Holder), this manages configuration for ontology parsing.
    This allows definition of domain specific parsing and filtering rules
    (once fully implemented)
    """
    global _CONF_OBJ_DICT
    app_name = request.application
    c = _CONF_OBJ_DICT.get(app_name)
    if c is None:
        from ConfigParser import SafeConfigParser
        c = SafeConfigParser({})
        c.read("applications/%s/private/localconfig" % request.application)
        _CONF_OBJ_DICT[app_name] = c
    return c


class ClassTarget(object):
    """
    receives notifications from the lxml 'etree' target (SAX-like) parser
    it currently ignores pretty much everything except OWL classes, and
    captures 'rdf:about', 'rdfs:label', IAO class comments (IAO:0000115),
    and tries not to stumble with owl restrictions and equivalent classes.
    The parser is pretty dependent on the owl rendering generated from the OWLAPI,
    which covers most or all of the OBO foundry ontologies, as well as the
    stuff generated by the owlbuilder tool.
    """
    def __init__(self):
        self.text = []
        self.class_list = []
        self.containerclass = None
    def start(self, tag, attrib):
        self.is_class = False
        self.is_label = False
        self.has_parent = False
        self.is_equivalent_class = False
        self.is_restriction = False
        self.is_class_comment = False
        if tag == OWL_CLASS:
            self.is_class = True
            if attrib:
                if RDF_ABOUT in attrib:
                    self.containerclass = {'about': attrib[RDF_ABOUT]}
        elif tag == OWL_EQUIVALENTCLASS:
            self.is_equivalent_class = True
        elif tag == OWL_RESTRICTION:
            self.is_restriction = True
        elif self.containerclass:
            if tag == RDFS_LABEL:
                self.is_label = True
            elif tag == OWL_SUBCLASS_OF:
                if RDF_RESOURCE in attrib:
                    self.containerclass['parent'] = attrib[RDF_RESOURCE]
                    self.has_parent = True
            elif tag == IAO_ANNOTATION:
                self.is_class_comment = True
    def end(self, tag):
        if tag == OWL_CLASS:
            if not self.is_equivalent_class:
                self.class_list.append(self.containerclass)
                self.containerclass = None
                self.is_class = False
        elif tag == OWL_EQUIVALENTCLASS:
            self.is_equivalent_class = False
        elif tag == OWL_RESTRICTION:
            self.is_restriction = False
        self.is_label = False
        self.has_parent = False
        self.is_class_comment = False
        pass
    def data(self, data):
        if self.containerclass:
            if self.is_label:
                self.mergedata('label',data)
            elif self.is_class_comment:
                self.mergedata('class_comment',data)
    def mergedata(self,tag,data):
        if tag in self.containerclass:
            self.containerclass[tag] = self.containerclass[tag] + data
        else:
            self.containerclass[tag] = data
    def close(self):
        return self.class_list
        
def update_ontology(ont,type_name):
    """
    builds term list from the specified ontology source
    ont - row from ontology_source table
    type_name - controlls processing - if NCBI taxonomy, this will
    only include terms subsumed by a particular node (e.g., all arachnids)
    filter terms based on labels (e.g., remove samples w/o possible behavior)
    """
    source_url = ont.source_url
    term_list = []
    if type_name == 'NCBI taxonomy':  #check symbolically
        term_list = load_from_url(source_url,build_ontology_tree,root=ARACHNID_NODE)
    elif type_name == 'OWL ontology':
        term_list = load_from_url(source_url,simple_builder)
    else:
        print 'unknown type name'
    return term_list

def load_from_url(ont_url,processor,root=None):
    """
    opens the url, parses the stream, then postprocesses (e.g., taxonomy filtering)
    ont_url - location of ontology text
    processor - function to pass over the list of terms returned by the parser
    root - argument for processors (e.g., root for taxonomic clade to use)
    """
    ontology_source=urlopen(ont_url)
    parser = etree.XMLParser(target = ClassTarget())
    results = etree.parse(ontology_source, parser)
    return processor(results,root)

def pplist(terms):
    """ debugging parser output"""
    import pprint
    pp = pprint.PrettyPrinter(indent=2)
    for term in terms:
        pp.pprint(term)

def process_tree(ont_tree):
    """debugging postprocessor output"""
    for child in ont_tree.getroot():
        print child.tag
    return ont_tree

ARACHNID_NODE = u'http://purl.obolibrary.org/obo/NCBITaxon_6854'

def build_ontology_tree(terms,root=None,label_filter=None):
    """
    postprocess tree returned from parsing taxonomic ontology
    terms - list of terms from parser
    root -  root of clade to save - everything else ignored
    Note: this currently returns dictionary of uri:term, rather than
    a simple list of terms.  Ought to fix to match the list of terms that
    comes in.
    """
    import pprint
    pp = pprint.PrettyPrinter(indent=2)
    roots = []
    tree_dict = dict()
    parent_dict = dict()
    for term in terms:
        if term_filter(term,label_filter):
            if 'about' in term:
                tree_dict[term['about']] = term
            if 'parent' in term:
                tp = term['parent']
                if tp in parent_dict and parent_dict[tp] != None:
                   t1 = parent_dict[tp]
                   t1.append(term)
                   parent_dict[tp] = t1
                else:
                   parent_dict[tp] = [term]
            else:
                roots.append(term)
    final_list = []
    children = [tree_dict[root]]
    while (children):
        child = children.pop(0)
        if 'label' in child:
            clabel = child['label']
            if not clabel.startswith('unclassified'):
                if 'about' in child:
                    final_list.append(tree_dict[child['about']])
                if child['about'] in parent_dict:
                    newchildren = parent_dict[child['about']]
                    children.extend(newchildren)
    return final_list

def simple_builder(terms,root=None):
    return terms

def term_filter(term,filter):
    if term:
        if filter:
            return filter(term)
        else:
            return True
    else:
        return False
            
def simple_label_filter(term):
    """ will reject labels suggesting sample identifiers from NCBI; returns boolean"""
    return True
    
def load_from_obo(ontology_name, processor,root=None):
    load_from_url(OBO_PURL_PREFIX+ontology_name+OWL_SUFFIX,processor,root)
    
def demo():
    '''For testing the owl parser'''
    load_from_obo('ncbitaxon',build_ontology_tree,root=ARACHNID_NODE)
    
    
def check_date(urlstr):
    """
    Opens a connection, tries to retrieve a last-modified in the headers
    """
    urlconn = urlopen(urlstr)
    urlinfo = urlconn.info()
    for header in urlinfo.headers:
        if header.startswith('Last-Modified: '):
            timestr = header[len('Last-Modified: '):len(header)]
            urlconn.close()
            return timestr
    else:        
        urlconn.close()
        return ''
